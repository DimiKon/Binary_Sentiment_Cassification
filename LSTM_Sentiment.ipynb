{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Sentiment Classifier"
      ],
      "metadata": {
        "id": "-6di3JjeR4Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import libraries"
      ],
      "metadata": {
        "id": "mMezv8uVUTPU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1vgF2nCL8hJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy import data, vocab\n",
        "from torchtext.data.utils import get_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed = 1234):\n",
        "    \"\"\"\n",
        "    Function to set the seed of the entire notebook for reproducibility of results\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed()"
      ],
      "metadata": {
        "id": "U_kwfvAprP89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check for GPU"
      ],
      "metadata": {
        "id": "cas32K1rUYcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "GcooRRR4ra0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2237b402-7280-46fb-b015-48fec8955e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available for running: \n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. DATA"
      ],
      "metadata": {
        "id": "PW8u5PxIUcD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## process data with Torchtext and TabularDataset"
      ],
      "metadata": {
        "id": "xmBAP5HXUhPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tokenizer\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# Initialize torchtext Field objects\n",
        "# Include_lengths tells the RNN how long the actual sequences are\n",
        "TEXT = data.Field(tokenize=tokenizer, lower=True, include_lengths= True)\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "# map data to fields\n",
        "fields = [('review', TEXT), ('sentiment', LABEL)]"
      ],
      "metadata": {
        "id": "DQ0_sMDQreK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TabularDataset to create training, validation, and testing torch datasets (from csv)\n",
        "# by applying field objects, which were initialized beforehand\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits( \n",
        "    path=\"/content/\", \n",
        "    train=\"data_train.csv\", \n",
        "    validation=\"data_dev.csv\",\n",
        "    test = \"data_test.csv\",\n",
        "    format=\"csv\", \n",
        "    skip_header=True, \n",
        "    fields=fields)\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of valid examples: {len(valid_data)}')\n",
        "print(f'Number of test examples: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PChwpJPkvVqe",
        "outputId": "4226a853-6e64-47fc-858d-011b49cdc312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 40999\n",
            "Number of valid examples: 4500\n",
            "Number of test examples: 4501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJp53UMKyoV5",
        "outputId": "2970931f-3dc0-43a6-f76e-e6d3c199905e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'review': ['this', 'is', 'in', 'short', 'the', 'tv', 'comedy', 'series', 'with', 'the', 'best', 'cast', 'ever', 'and', 'the', 'most', 'likable', 'also', 'each', 'one', 'of', 'them', 'is', 'first', 'hand', 'comedy', 'actor', 'know', 'only', 'one', 'tv', 'series', 'which', 'was', 'better', 'e', 'moonlighting', 'that', 'one', 'had', 'willis', 'as', 'lead', 'yet', 'it', 'had', 'willis', 'only', 'while', 'the', 'king', 'of', 'queens', 'has', 'pocketful', 'of', 'actors', 'that', 'are', 'as', 'fine', 'as', 'one', 'can', 'enjoy', 'kevin', 'james', 'leah', 'remini', 'jerry', 'stiller', 'patton', 'oswalt', 'nicole', 'sullivan', 'victor', 'williams', 'gary', 'valentine', 'and', 'even', 'all', 'the', 'rest', 'of', 'them', 'spontaneously', 'and', 'continually', 'and', 'promptly', 'liked', 'it', 'advancing', 'age', 'did', 'not', 'spoil', 'the', 'fun', 'anyway', 'in', 'few', 'words', 'the', 'series', 'is', 'intelligent', 'and', 'original', 'miraculously', 'spared', 'of', 'the', 'current', 'tv', 'stupidity', 'and', 'garbage', 'it', 'is', 'politically', 'incorrect', 'and', 'does', 'not', 'court', 'the', 'minorities', 'in', 'the', 'usual', 'disgusting', 'way', 'the', 'comic', 'is', 'very', 'palatable', 'and', 'savory', 'read', 'mostly', 'approvingly', 'few', 'imdb', 'writers', 'and', 'sometimes', 'they', 'write', 'about', 'their', 'favorite', 'shows', 'yet', 'though', 'these', 'writers', 'are', 'several', 'did', 'not', 'encountered', 'at', 'any', 'of', 'them', 'the', 'slightest', 'mention', 'of', 'my', 'favorite', 'tv', 'shows', 'but', 'it', 'is', 'true', 'that', 'the', 'critics', 'one', 'likes', 'are', 'not', 'those', 'with', 'whom', 'he', 'finds', 'himself', 'in', 'complete', 'approval', 'but', 'those', 'who', 'at', 'least', 'offer', 'common', 'basis', 'for', 'disapproval', 'which', 'are', 'mainly', 'wild', 'wild', 'west', 'moonlighting', 'queens', 'fantomas', 'the', '#', '#', 's', 'twilight', 'zone', 'bradbury', 'tv', 'show', 'and', 'sandokan', 'most', 'of', 'them', 'have', 'seen', 'when', 'was', '#', '#', '#', '#', 'yrs', 'about', 'few', 'of', 'them', 'have', 'written', 'and', 'execrably'], 'sentiment': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build Vocabulary (with or without Glove pretrained embeddings)"
      ],
      "metadata": {
        "id": "AAjC2Tf-aTzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary objects (convert words into integers) for the training set\n",
        "\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "\n",
        "# Without Glove (uncomment to run)\n",
        "#TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "# With Glove pretrained (uncomment to run)\n",
        "#TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE,vectors = \"glove.6B.100d\",unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHdwUxubudCL",
        "outputId": "384b564f-1f73-4a8b-f240-bdd62df134ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:22<00:00, 17857.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFUpijcjusrm",
        "outputId": "778041dc-c49f-450d-eb07-f958a8555647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 20002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most frequent tokens\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cquP8Xy5yfIj",
        "outputId": "f5ff9439-fc00-47c7-ba57-d239e71a97d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 547738),\n",
              " ('and', 265893),\n",
              " ('of', 237095),\n",
              " ('to', 219592),\n",
              " ('is', 213467),\n",
              " ('it', 156349),\n",
              " ('in', 153092),\n",
              " ('this', 123727),\n",
              " ('that', 118573),\n",
              " ('#', 109039)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initialize iterators that batch examples of similar lengths together with BucketIterator"
      ],
      "metadata": {
        "id": "DYw9-tUiVUH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# sort_within_batch sorts all the tensors within a batch by their lengths\n",
        "# BucketIterator groups input data (reviews) of similar lengths together for minimized padding in each batch\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.review),\n",
        "    sort_within_batch = True)"
      ],
      "metadata": {
        "id": "lko1O-GA2kB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. MODEL: bidirectional stacked RNN with LSTM cell"
      ],
      "metadata": {
        "id": "jCMvizZPVmBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentiment_Classifier_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \"\"\"\n",
        "        Initialize the RNN model that will be used \n",
        "        to perform sentiment analysis, by setting up the layers\n",
        "\n",
        "        vocab_size - input size (unique words in the vocabulary)\n",
        "        embedding_dim - size of the dense word vectors (number of features for each time step)\n",
        "        hidden_dim - size of the hidden states\n",
        "        output_dim - number of classes\n",
        "        n_layers - number of multi-layer RNN\n",
        "        bidirectional - boolean - use both directions of LSTM\n",
        "        dropout - dropout probability\n",
        "        pad_idx -  string representing the pad token\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Feed the reviews in the embedding layer\n",
        "        # padding_idx set to not learn the emedding for the <pad> token - irrelevant to determining sentiment\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        # 2. LSTM layer\n",
        "        # returns the output and a tuple of the final hidden state and final cell state\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                               hidden_dim, \n",
        "                               num_layers=n_layers,\n",
        "                               bidirectional=bidirectional,\n",
        "                               dropout=dropout)\n",
        "        \n",
        "        # 3. Fully-connected layer\n",
        "        # Final hidden state has both a forward and a backward component concatenated together\n",
        "        # The size of the input to the nn.Linear layer is twice that of the hidden dimension size\n",
        "        self.predictor = nn.Linear(hidden_dim*2, output_dim)\n",
        "\n",
        "        # Initialize dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def forward(self, text, text_lengths):\n",
        "        \"\"\"\n",
        "        Function to call when data is fed into the model\n",
        "\n",
        "        text - [review length, batch size]\n",
        "        text_lengths - length of batch\n",
        "        \"\"\"\n",
        "\n",
        "        # embedded = [sentence len, batch size, emb dim]\n",
        "        embedded = self.dropout(self.embedding(text))    \n",
        "\n",
        "        # Pack the embeddings - cause RNN to only process non-padded elements\n",
        "        # Speeds up computation\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\n",
        "\n",
        "        # output of lstm\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "\n",
        "        # unpack sequence - transform packed sequence to a tensor\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        # output = [sentence len, batch size, hid dim * num directions]\n",
        "        # output over padding tokens are zero tensors\n",
        "        \n",
        "        # hidden = [num layers * num directions, batch size, hid dim]\n",
        "        # cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        # Get the final layer forward and backward hidden states  \n",
        "        # concat the final forward and backward hidden layers and apply dropout\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "        # hidden = [batch size, hid dim * num directions]\n",
        "\n",
        "        # return output after it had passed through a fully-connected layer\n",
        "        return self.predictor(hidden)"
      ],
      "metadata": {
        "id": "aN1zzvUf9F_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CREATE MODEL "
      ],
      "metadata": {
        "id": "QoyLsGVpWwkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "# 2 layers of biLSTM\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "# Dropout probability\n",
        "DROPOUT = 0.2\n",
        "# Get pad token index from vocab\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# Create an instance of LSTM class\n",
        "model = Sentiment_Classifier_LSTM(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL,\n",
        "            DROPOUT,\n",
        "            PAD_IDX)"
      ],
      "metadata": {
        "id": "24LCLYla95zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run this cell only if pretrained embeddings are used (else skip it and go on to the next one)"
      ],
      "metadata": {
        "id": "R37pUKqUeQiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the pre-trained word embeddings into the embedding layer\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "# [vocab size, embedding dim]\n",
        "print(pretrained_embeddings.shape)\n",
        "\n",
        "# copy the pre-trained word embeddings into the embedding layer\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
        "print(model.embedding.weight)\n",
        "\n",
        "\n",
        "# Initialize <unk> and <pad> both to all zeros - irrelevant for sentiment analysis\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "# Setting row in the embedding weights matrix to zero using the token index\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "# Freeze weights\n",
        "model.embedding.weight.requires_grad=False\n",
        "\n",
        "print(model.embedding.weight)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Nl8sN4ehQx",
        "outputId": "28fb062f-0d91-41f2-e8f7-3cda3d538bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20002, 100])\n",
            "Parameter containing:\n",
            "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
            "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.2215, -0.0018,  0.7275,  ..., -0.6555, -0.4417,  0.4873],\n",
            "        [ 0.1070,  0.1660,  0.8914,  ..., -0.6368,  0.2450,  1.0492],\n",
            "        [-0.3429,  1.0147,  0.3113,  ..., -0.0477,  0.2576,  0.1918]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.2215, -0.0018,  0.7275,  ..., -0.6555, -0.4417,  0.4873],\n",
            "        [ 0.1070,  0.1660,  0.8914,  ..., -0.6368,  0.2450,  1.0492],\n",
            "        [-0.3429,  1.0147,  0.3113,  ..., -0.0477,  0.2576,  0.1918]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize <unk> and <pad> both to all zeros - irrelevant for sentiment analysis\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "# Setting row in the embedding weights matrix to zero using the token index\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQV5DEg7-PkE",
        "outputId": "ed4e6f0a-6766-417a-db04-6e40fc76e7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.7289, -0.7336,  1.5624,  ..., -0.5592, -0.4480, -0.6476],\n",
            "        ...,\n",
            "        [-0.4019, -1.6036,  0.7195,  ...,  0.8753,  1.2358,  0.2100],\n",
            "        [-0.2028,  0.4162, -0.0036,  ...,  0.7825,  0.1047,  1.1312],\n",
            "        [-0.0591,  0.4980, -0.3215,  ..., -0.6867,  0.5813,  1.2588]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer used to update the weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
        "\n",
        "# Loss function: binary cross entropy with logits\n",
        "# It restricts the predictions to a number between 0 and 1 using the logit function\n",
        "# then use the bound scarlar to calculate the loss using binary cross entropy\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Use GPU\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "ps56UZpL-sV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "def batch_accuracy(predictions, label):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch.\n",
        "\n",
        "    predictions - float\n",
        "    label - 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "    # Round predictions to the closest integer using the sigmoid function\n",
        "    preds = torch.round(torch.sigmoid(predictions))\n",
        "    # If prediction is equal to label\n",
        "    correct = (preds == label).float()\n",
        "    # Average correct predictions\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def timer(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Returns the minutes and seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    time = end_time - start_time\n",
        "    mins = int(time / 60)\n",
        "    secs = int(time - (mins * 60))\n",
        "\n",
        "    return mins, secs"
      ],
      "metadata": {
        "id": "9zXhGSVm-wMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate training loss and accuracy.\n",
        "\n",
        "    iterator - train iterator\n",
        "    \"\"\"\n",
        "    \n",
        "    # Cumulated Training loss\n",
        "    training_loss = 0.0\n",
        "    # Cumulated Training accuracy\n",
        "    training_acc = 0.0\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # For each batch in the training iterator\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # batch.text is a tuple (tensor, len of seq)\n",
        "        text, text_lengths = batch.review\n",
        "        \n",
        "        # 2. Compute the predictions\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        # 3. Compute loss\n",
        "        loss = criterion(predictions, batch.sentiment)\n",
        "        \n",
        "        # Compute accuracy\n",
        "        accuracy = batch_accuracy(predictions, batch.sentiment)\n",
        "        \n",
        "        # 4. Use loss to compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # 5. Use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "        \n",
        "        training_loss += loss.item()\n",
        "        training_acc += accuracy.item()\n",
        "    \n",
        "    # Return the loss and accuracy, averaged across each epoch\n",
        "    # len of iterator = num of batches in the iterator\n",
        "    return training_loss / len(iterator), training_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Function to evaluate the loss and accuracy of validation and test sets.\n",
        "\n",
        "    iterator - validation or test iterator\n",
        "    \"\"\"\n",
        "    \n",
        "    # Cumulated Training loss\n",
        "    eval_loss = 0.0\n",
        "    # Cumulated Training accuracy\n",
        "    eval_acc = 0\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Don't calculate the gradients\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.review\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.sentiment)\n",
        "            \n",
        "            accuracy = batch_accuracy(predictions, batch.sentiment)\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            eval_acc += accuracy.item()\n",
        "        \n",
        "    return eval_loss / len(iterator), eval_acc / len(iterator)"
      ],
      "metadata": {
        "id": "tpkDAbnw-z5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. TRAINING"
      ],
      "metadata": {
        "id": "U_NBZjWzXYE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Evaluate training loss and accuracy\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    # Evaluate validation loss and accuracy\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    mins, secs = timer(start_time, end_time)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # Save the parameters of the model\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(\"Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Total Time: {}m {}s\".format(mins, secs))\n",
        "    print(\"\\t Train Loss {} | Train Accuracy: {}%\".format(round(train_loss, 2), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss {} | Validation Accuracy: {}%\".format(round(valid_loss, 2), round(valid_acc*100, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dmkwsD8-4my",
        "outputId": "74825ad4-f047-49c1-e098-5f5a9a91fcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "\t Total Time: 1m 13s\n",
            "\t Train Loss 0.57 | Train Accuracy: 67.53%\n",
            "\t Validation Loss 0.44 | Validation Accuracy: 79.29%\n",
            "Epoch 2:\n",
            "\t Total Time: 1m 12s\n",
            "\t Train Loss 0.38 | Train Accuracy: 83.31%\n",
            "\t Validation Loss 0.34 | Validation Accuracy: 85.72%\n",
            "Epoch 3:\n",
            "\t Total Time: 1m 12s\n",
            "\t Train Loss 0.34 | Train Accuracy: 85.69%\n",
            "\t Validation Loss 0.3 | Validation Accuracy: 88.06%\n",
            "Epoch 4:\n",
            "\t Total Time: 1m 12s\n",
            "\t Train Loss 0.31 | Train Accuracy: 86.7%\n",
            "\t Validation Loss 0.29 | Validation Accuracy: 88.28%\n",
            "Epoch 5:\n",
            "\t Total Time: 1m 12s\n",
            "\t Train Loss 0.31 | Train Accuracy: 87.06%\n",
            "\t Validation Loss 0.3 | Validation Accuracy: 87.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. EVALUATING"
      ],
      "metadata": {
        "id": "gyGbFAlpXg0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model with the best validation loss\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "# Evaluate test loss and accuracy\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(\"Test Loss: {} | Test Acc: {}%\".format(round(test_loss, 2), round(test_acc*100, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "082DiSmNC6yz",
        "outputId": "cdc430a2-e829-4327-ab20-6bfb3e0be892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.31 | Test Acc: 87.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, text):\n",
        "    \"\"\"\n",
        "    Function to predict the sentiment given a tweet\n",
        "    \"\"\"\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Index tokens by converting to the integer representation from the vocabulary\n",
        "    indexed_tokens = [TEXT.vocab.stoi[t] for t in text]\n",
        "    # Get the length of the text\n",
        "    length = [len(indexed_tokens)]\n",
        "    # Convert the indices to a tensor\n",
        "    tensor = torch.LongTensor(indexed_tokens).to(device)\n",
        "    # Add a batch dimension by unsqueezeing\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    # Convert the length into a tensor\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get prediction\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    # Binarize prediction\n",
        "    y_pred_tag = torch.round(prediction)\n",
        "\n",
        "    return y_pred_tag"
      ],
      "metadata": {
        "id": "x8Z15POsDEfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use testing dataset for inference\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for i in range(4501):\n",
        "  with torch.no_grad():\n",
        "    text = test_data[i].review\n",
        "    label = test_data[i].sentiment\n",
        "    preds = predict(model, text)\n",
        "    predicted_labels.append(preds.cpu().numpy())\n",
        "    true_labels +=[int(label)]\n",
        "\n",
        "predicted_labels = [a.squeeze().tolist() for a in predicted_labels]\n",
        "predicted_labels = [int(a)for a in predicted_labels]\n",
        "\n",
        "#print(predicted_labels)\n",
        "#print(true_labels)"
      ],
      "metadata": {
        "id": "1RAktMmtDoza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "KUGFLuNwX8PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics report\n",
        "target_names = ['class 0_negative', 'class 1_positive']\n",
        "scores = metrics.classification_report(true_labels, predicted_labels, target_names=target_names)\n",
        "print('best\\n')\n",
        "print(scores)\n",
        "mat = confusion_matrix(true_labels, predicted_labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "YN5K5GtrKJtp",
        "outputId": "6445fe53-4d79-497f-f269-0c099d86e1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best\n",
            "\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "class 0_negative       0.87      0.88      0.87      2250\n",
            "class 1_positive       0.88      0.87      0.87      2251\n",
            "\n",
            "        accuracy                           0.87      4501\n",
            "       macro avg       0.87      0.87      0.87      4501\n",
            "    weighted avg       0.87      0.87      0.87      4501\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(91.68, 0.5, 'predicted label')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}